{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c28cd9d-c1c1-4a5b-896b-bfbe8419835f",
   "metadata": {},
   "source": [
    "# My ETL Package\n",
    "\n",
    "This package implements a simple **Extract‚ÄìTransform‚ÄìLoad (ETL)** pipeline using Python.  \n",
    "It processes CSV files from an input directory, transforms and integrate them into a single dataset, saves the results to an output directory, and loads the final dataset into a PostgreSQL database.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f4e58d-3d2e-4afd-802a-e26c01da34c2",
   "metadata": {},
   "source": [
    "## üîë Pre-requisites\n",
    "\n",
    "1. **Working Python environment**  \n",
    "   Make sure you have a working Python environment set up, either locally or in Jupyter.  \n",
    "   If you haven‚Äôt already, follow this guide:  \n",
    "   [Setting up a Basic Python Development Environment](https://medium.com/@khhaledahmaad/setting-up-a-basic-python-development-environment-fd67e749825e)\n",
    "\n",
    "2. **PostgreSQL database**  \n",
    "   Ensure you have a PostgreSQL database installed and configured.  \n",
    "   If you don‚Äôt have one already, download and install both PostgreSQL and PgAdmin from the official sources below, then use PgAdmin to create your first database:\n",
    "\n",
    "   - [Download PostgreSQL](https://www.postgresql.org/download/)  \n",
    "   - [Download PgAdmin](https://www.pgadmin.org/download/)  \n",
    "\n",
    "   Once installed, you can follow this step-by-step guide to create a new database in PgAdmin:  \n",
    "   [Creating a Database using PgAdmin](https://www.tutorialsteacher.com/postgresql/create-database)\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Installation\n",
    "\n",
    "Install directly from **PyPI**:\n",
    "\n",
    "```bash\n",
    "pip install my-etl-package\n",
    "````\n",
    "\n",
    "Set up your environment variables in a `.env` file (required for PostgreSQL connection):\n",
    "\n",
    "```env\n",
    "DB_HOST=localhost\n",
    "DB_PORT=5432\n",
    "DB_NAME=mydatabase\n",
    "DB_USER=myuser\n",
    "DB_PASSWORD=mypassword\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Package Contents\n",
    "\n",
    "After installation, you can inspect the available functions:\n",
    "\n",
    "```python\n",
    "import my_etl_package\n",
    "help(my_etl_package)\n",
    "```\n",
    "\n",
    "Typical contents:\n",
    "\n",
    "```\n",
    "NAME\n",
    "    my_etl_package\n",
    "\n",
    "PACKAGE CONTENTS\n",
    "    utils\n",
    "\n",
    "FUNCTIONS\n",
    "    read_csv(file_path: pathlib.Path) -> pandas.DataFrame\n",
    "    transform_data(dfs: List[pandas.DataFrame]) -> pandas.DataFrame\n",
    "    write_csv(df: pandas.DataFrame, output_path: pathlib.Path) -> None\n",
    "    load_to_db(df: pd.DataFrame, table_name: str, engine: sqlalchemy.engine.Engine) -> None\n",
    "```\n",
    "\n",
    "Utilities inside `my_etl_package.utils`:\n",
    "\n",
    "```\n",
    "FUNCTIONS\n",
    "    list_csv_files(directory_path: pathlib.Path) -> List[pathlib.Path]\n",
    "    PostgresConnector().get_db_connection() -> sqlalchemy.engine.Engine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Data Directory Structure (Recommended but not Mandatory)\n",
    "\n",
    "When running locally, organize your data as follows (relative to your **current working directory**):\n",
    "\n",
    "```\n",
    "pwd/\n",
    "‚îú‚îÄ‚îÄ .env              # Environment variables (PostgreSQL credentials)\n",
    "‚îî‚îÄ‚îÄ data/\n",
    "    ‚îú‚îÄ‚îÄ raw/          # Place input CSV files here\n",
    "    ‚îî‚îÄ‚îÄ processed/    # Processed output CSVs will be written here\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc11d-85dd-434c-9954-082fbaf5ed04",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Usage\n",
    "\n",
    "### 1. Use Methods Individually\n",
    "\n",
    "#### 1.1. List all CSV files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6ba8e4-bfd1-4148-9d8f-e5c9108ed035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('C:/Users/khhal/Documents/data/raw/103_semester_2_week_1_raw.csv'), WindowsPath('C:/Users/khhal/Documents/data/raw/104_semester_2_week_2_raw.csv')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from my_etl_package.utils import list_csv_files\n",
    "\n",
    "input_directory = Path().absolute() / \"data/raw\"\n",
    "files = list_csv_files(input_directory)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85878bc5-be46-4d2c-9dec-51fe944acaad",
   "metadata": {},
   "source": [
    "#### 1.2. Read a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52b14d70-831b-4e3f-a2c1-90d6c9a70e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>message</th>\n",
       "      <th>status</th>\n",
       "      <th>date_sent</th>\n",
       "      <th>student_id</th>\n",
       "      <th>course_code</th>\n",
       "      <th>student_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447440049121</td>\n",
       "      <td>3821656</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-01-25 13:22:10+00:00</td>\n",
       "      <td>3821656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>447440049121</td>\n",
       "      <td>3821656 103</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-01-25 12:26:28+00:00</td>\n",
       "      <td>3821656</td>\n",
       "      <td>103.0</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           from      message    status                  date_sent  student_id  \\\n",
       "0  447440049121      3821656  received  2023-01-25 13:22:10+00:00     3821656   \n",
       "1  447440049121  3821656 103  received  2023-01-25 12:26:28+00:00     3821656   \n",
       "\n",
       "   course_code  student_name  \n",
       "0          NaN  Khaled Ahmed  \n",
       "1        103.0  Khaled Ahmed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_etl_package import read_csv\n",
    "\n",
    "df1 = read_csv(files[0])\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97fd28e6-b4e4-4a7b-b360-7c28307228e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>message</th>\n",
       "      <th>status</th>\n",
       "      <th>date_sent</th>\n",
       "      <th>student_id</th>\n",
       "      <th>course_code</th>\n",
       "      <th>student_name</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.474400e+11</td>\n",
       "      <td>3821656 104</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-02-02 13:24:02+00:00</td>\n",
       "      <td>3821656.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           from      message    status                  date_sent  student_id  \\\n",
       "0  4.474400e+11  3821656 104  received  2023-02-02 13:24:02+00:00   3821656.0   \n",
       "1           NaN          NaN       NaN                        NaN         NaN   \n",
       "\n",
       "   course_code  student_name  session  \n",
       "0        104.0  Khaled Ahmed      NaN  \n",
       "1          NaN           NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_etl_package import read_csv\n",
    "\n",
    "df2 = read_csv(files[1])\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030df1a8-34f9-4d21-950f-eb0191149ae9",
   "metadata": {},
   "source": [
    "#### 1.3. Transform multiple CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "168dcd9e-f177-4452-95a3-3b7ead6f27bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>message</th>\n",
       "      <th>status</th>\n",
       "      <th>date_sent</th>\n",
       "      <th>student_id</th>\n",
       "      <th>course_code</th>\n",
       "      <th>student_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.474400e+11</td>\n",
       "      <td>3821656</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-01-25 13:22:10+00:00</td>\n",
       "      <td>3821656.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.474400e+11</td>\n",
       "      <td>3821656 103</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-01-25 12:26:28+00:00</td>\n",
       "      <td>3821656.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.474400e+11</td>\n",
       "      <td>3821656 104</td>\n",
       "      <td>received</td>\n",
       "      <td>2023-02-02 13:24:02+00:00</td>\n",
       "      <td>3821656.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>Khaled Ahmed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           from      message    status                  date_sent  student_id  \\\n",
       "0  4.474400e+11      3821656  received  2023-01-25 13:22:10+00:00   3821656.0   \n",
       "1  4.474400e+11  3821656 103  received  2023-01-25 12:26:28+00:00   3821656.0   \n",
       "2  4.474400e+11  3821656 104  received  2023-02-02 13:24:02+00:00   3821656.0   \n",
       "\n",
       "   course_code  student_name  \n",
       "0          NaN  Khaled Ahmed  \n",
       "1        103.0  Khaled Ahmed  \n",
       "2        104.0  Khaled Ahmed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_etl_package import transform_data\n",
    "\n",
    "combined_df = transform_data([df1, df2])\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681fc8f-271a-4433-9105-65567ca1fb82",
   "metadata": {},
   "source": [
    "#### 1.4. Write processed DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "248f5080-03e0-4746-8c94-ee533ab0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from my_etl_package import write_csv\n",
    "\n",
    "output_path = Path().absolute() / \"data/processed\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "output_file = output_path / \"processed.csv\"\n",
    "write_csv(combined_df, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e70f7d7-ef50-4565-bf82-3754baad9d01",
   "metadata": {},
   "source": [
    "#### 1.5. Load DataFrame into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "819762c1-6491-4aa4-aa76-336e959a5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:my_etl_package.utils.connect_db:PostgresConnector initialized with loaded credentials.\n",
      "INFO:my_etl_package.utils.connect_db:Database engine generated for: postgresql://postgres:Khal8891@localhost:5434/postgres.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from my_etl_package.utils import PostgresConnector\n",
    "from my_etl_package import load_to_db\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# If .env is in a different location, specify the path:\n",
    "# load_dotenv('./some_other_location/.env')\n",
    "\n",
    "table_name = \"etl_pipeline_processed\"\n",
    "load_to_db(combined_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580693d-13f7-4759-8ffc-ed09224d06dc",
   "metadata": {},
   "source": [
    "### 2. Run the Full ETL Pipeline\n",
    "\n",
    "Here‚Äôs an end-to-end pipeline script:\n",
    "\n",
    "In `etl_pipeline.py` (name as you wish) in the current working directory ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fba5418-6287-468b-b9d3-1c15ebb7b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Environment variables loaded.\n",
      "INFO:root:Output directory set to: C:\\Users\\khhal\\Documents\\data\\processed\n",
      "INFO:root:Starting ETL pipeline...\n",
      "INFO:root:Using table: etl_pipeline_processed\n",
      "INFO:root:Looking for CSV files in: C:\\Users\\khhal\\Documents\\data\\raw\n",
      "INFO:root:Found 2 CSV files.\n",
      "INFO:root:Transforming data...\n",
      "INFO:root:Data transformation complete.\n",
      "INFO:root:Writing processed data to CSV...\n",
      "INFO:root:Data written to CSV.\n",
      "INFO:root:Loading data into PostgreSQL...\n",
      "INFO:my_etl_package.utils.connect_db:PostgresConnector initialized with loaded credentials.\n",
      "INFO:my_etl_package.utils.connect_db:Database engine generated for: postgresql://postgres:Khal8891@localhost:5434/postgres.\n",
      "INFO:root:Data successfully loaded into PostgreSQL.\n",
      "INFO:root:ETL pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from my_etl_package.utils import list_csv_files, PostgresConnector\n",
    "from my_etl_package import read_csv, transform_data, write_csv, load_to_db\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "logging.info(\"Environment variables loaded.\")\n",
    "\n",
    "# Set up workspace\n",
    "base = Path().absolute() / \"data\"\n",
    "input_directory = base / \"raw\"\n",
    "output_directory = base / \"processed\"\n",
    "output_filename = \"etl_pipeline_processed.csv\"\n",
    "output_path = output_directory / output_filename\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_directory.mkdir(exist_ok=True)\n",
    "logging.info(f\"Output directory set to: {output_directory}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting ETL pipeline...\")\n",
    "\n",
    "    # Configuration\n",
    "    table_name = \"etl_pipeline_processed\"\n",
    "    logging.info(f\"Using table: {table_name}\")\n",
    "\n",
    "    # Extract\n",
    "    logging.info(f\"Looking for CSV files in: {input_directory}\")\n",
    "    file_paths = list_csv_files(input_directory)\n",
    "    logging.info(f\"Found {len(file_paths)} CSV files.\")\n",
    "\n",
    "    # Read\n",
    "    dfs = (read_csv(f) for f in file_paths)\n",
    "\n",
    "    # Transform\n",
    "    logging.info(\"Transforming data...\")\n",
    "    combined_df = transform_data(dfs)\n",
    "    logging.info(\"Data transformation complete.\")\n",
    "\n",
    "    # Load - write to CSV\n",
    "    logging.info(\"Writing processed data to CSV...\")\n",
    "    write_csv(combined_df, output_path)\n",
    "    logging.info(\"Data written to CSV.\")\n",
    "\n",
    "    # Load - load into Postgres\n",
    "    logging.info(\"Loading data into PostgreSQL...\")\n",
    "    load_to_db(combined_df, table_name)\n",
    "    logging.info(\"Data successfully loaded into PostgreSQL.\")\n",
    "\n",
    "    logging.info(\"ETL pipeline finished.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6baecc-62e3-40b1-8ba7-998e33bd7005",
   "metadata": {},
   "source": [
    "Run it:\n",
    "\n",
    "```bash\n",
    "python etl_pipeline.py\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cfc7f-861c-44ea-b52f-15f08273fe99",
   "metadata": {},
   "source": [
    "## ‚úÖ Features\n",
    "\n",
    "* üîé Automatically detects all CSV files in `data/raw/`\n",
    "* üõ†Ô∏è Cleans and transforms raw datasets\n",
    "* üíæ Stores processed results in `data/processed/`\n",
    "* üóÑÔ∏è Loads final output into a PostgreSQL table\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "* Ensure PostgreSQL is running and accessible with the credentials in your `.env` file.\n",
    "* Place your input CSV files in the same input directory before running the pipeline.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_new",
   "language": "python",
   "name": "etl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
